{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d59d527f-1100-45ff-b051-5f7c9029d94d",
      "metadata": {},
      "source": [
        "# Queries with and without Azure OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba",
      "metadata": {},
      "source": [
        "Now that we have our Search Engine loaded and running, we are going to try some example queries and then use Azure OpenAI service to see if we can get even better results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08",
      "metadata": {},
      "source": [
        "## Set up variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f",
      "metadata": {
        "gather": {
          "logged": 1679978716488
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "from app.embeddings import OpenAIEmbeddings\n",
        "from app.prompts import STUFF_PROMPT, REFINE_PROMPT, REFINE_QUESTION_PROMPT\n",
        "from app.credentials import (\n",
        "    API_VERSION,\n",
        "    DATASOURCE_CONNECTION_STRING,\n",
        "    AZURE_SEARCH_ENDPOINT,\n",
        "    AZURE_SEARCH_KEY,\n",
        "    COG_SERVICES_NAME,\n",
        "    COG_SERVICES_KEY,\n",
        "    AZURE_OPENAI_ENDPOINT,\n",
        "    AZURE_OPENAI_KEY,\n",
        "    AZURE_OPENAI_TYPE,\n",
        "    AZURE_OPENAI_API_VERSION\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc",
      "metadata": {
        "gather": {
          "logged": 1679978716682
        }
      },
      "outputs": [],
      "source": [
        "# Setup the Payloads header\n",
        "headers = {'Content-Type': 'application/json','api-key': AZURE_SEARCH_KEY}\n",
        "params = {'api-version': API_VERSION}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9297d29b-1f61-4dce-858e-bf4272172dba",
      "metadata": {},
      "source": [
        "## Without Azure OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5a46e2d3-298a-4708-83de-9e108b1a117a",
      "metadata": {
        "gather": {
          "logged": 1679978716877
        }
      },
      "outputs": [],
      "source": [
        "# Index that we are going to query (from Notebook 01)\n",
        "index_name = \"cogsrch-index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead",
      "metadata": {
        "gather": {
          "logged": 1679978717065
        }
      },
      "outputs": [],
      "source": [
        "QUESTION = \"How Markov chains work?\" \n",
        "\n",
        "# Try questions that you think might be answered or addressed in computer science papers in 2020-2021\n",
        "# And compare the results with the open version of ChatGPT\n",
        "# The idea is that the answers using Azure OpenAI only looks at the information contained on these publications.\n",
        "\n",
        "# For Example:\n",
        "# What is CLP?\n",
        "# How Markov chains work?\n",
        "# What are some examples of reinforcement learning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f5c7e9f-42b9-4081-8975-648e4b31e175",
      "metadata": {
        "gather": {
          "logged": 1679978717322
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://search-ehfwemwhjdyek.search.windows.net/indexes/cogsrch-index/docs?api-version=2021-04-30-Preview&search=How Markov chains work?&select=pages&$top=5&queryLanguage=en-us&queryType=semantic&semanticConfiguration=my-semantic-config&$count=true&speller=lexicon&answers=extractive|count-3&captions=extractive|highlight-true&highlightPreTag=%3Cspan%20style%3D%22background-color%3A%20%23f5e8a3%22%3E&highlightPostTag=%3C%2Fspan%3E\n",
            "200\n",
            "Results Found: 9100, Results Returned: 5\n"
          ]
        }
      ],
      "source": [
        "url = AZURE_SEARCH_ENDPOINT + '/indexes/'+ index_name + '/docs'\n",
        "url += '?api-version={}'.format(API_VERSION)\n",
        "url += '&search={}'.format(QUESTION)\n",
        "url += '&select=pages'\n",
        "url += '&$top=5'\n",
        "url += '&queryLanguage=en-us'\n",
        "url += '&queryType=semantic'\n",
        "url += '&semanticConfiguration=my-semantic-config'\n",
        "url += '&$count=true'\n",
        "url += '&speller=lexicon'\n",
        "url += '&answers=extractive|count-3'\n",
        "url += '&captions=extractive|highlight-true'\n",
        "url += '&highlightPreTag=' + urllib.parse.quote('<span style=\"background-color: #f5e8a3\">', safe='')\n",
        "url += '&highlightPostTag=' + urllib.parse.quote('</span>', safe='')\n",
        "\n",
        "resp = requests.get(url, headers=headers)\n",
        "print(url)\n",
        "print(resp.status_code)\n",
        "\n",
        "search_results = resp.json()\n",
        "print(\"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "015f0259-6609-4032-b8ae-09e95c69cb55",
      "metadata": {
        "gather": {
          "logged": 1679978717889
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h4>Top Answers</h4>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h5>Answer - score: 0.779296875</h5>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Markov chain Monte Carlo (MCMC) does this by constructing a Markov chain that uses the underlying distribution as its stationary distribution. This enables the simulation of the stochastic process for non-standard distributions, while ensuring that the SLLN will hold. As an illustration of the MCMC we will discuss the Metropolis algorithm [11]."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h4>Top Results</h4>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h5>0403040v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.191192626953125</h5>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Theorem 1 Let M ′ be the Markov chain defined over the space A′ of all connected  acyclic digraphs, together with the transitions defined by the rules (T ′ 1) and (T2). The chain M ′ is irreducible, that is, given two connected acyclic digraphs G and  H, there exists in M ′ a sequence of transitions G = G0 → G1, G1 → G2, . . ."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h5>0603059v2.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.16461181640625</h5>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "here the markov chain is defined by a 2×2 stochastic matrix π = [πij ] (the reader should not confuse π with the 4 × 4 matrix ∆:          π00(1 − ε) π00ε π01(1 − ε) π01ε π00(1 − ε) π00ε π01(1 − ε) π01ε π10(1 − ε) π10ε π11(1 − ε) π11ε π10(1 − ε) π10ε π11(1 − ε) π11ε          ,  which defines the hidden markov chain via a deterministic function) …"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h5>0603059v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.145233154296875</h5>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "A function Z = {Z∞ −∞} of the Markov chain Y with the form Z = Φ(Y )  is called a hidden Markov chain; here Φ is a finite valued function defined on {1, 2, · · · , B}, taking values in {1, 2, · · · , A}."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h5>0610134v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.058868408203125</h5>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "A Markov Modulated Process based upon an infinite  Markov chain is described. The work described is motivated by applications in telecommunications  where LRD is a known property of time-series measured on the internet."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h5>0309016v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.0449981689453125</h5>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Markov chain Monte Carlo (MCMC) does this by constructing a Markov chain that uses the underlying distribution as its stationary distribution. This enables the simulation of the stochastic process for non-standard distributions, while ensuring that the SLLN will hold. As an illustration of the MCMC we will discuss the Metropolis algorithm [11]."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Answers from semantic Search\n",
        "display(HTML('<h4>Top Answers</h4>'))\n",
        "for result in search_results['@search.answers']:\n",
        "    if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
        "        display(HTML('<h5>' + 'Answer - score: ' + str(result['score']) + '</h5>'))\n",
        "        display(HTML(result['text']))\n",
        "\n",
        "        \n",
        "# Results from key-word search\n",
        "file_content = dict()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "display(HTML('<h4>Top Results</h4>'))\n",
        "for result in search_results['value']:\n",
        "    if result['@search.rerankerScore'] > 0.4: # Show results that are at least 10% of the max possible score=4\n",
        "        display(HTML('<h5>' + result['metadata_storage_name'] + '&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: ' + str(result['@search.rerankerScore']) + '</h5>'))\n",
        "        display(HTML(result['@search.captions'][0]['text']))\n",
        "        file_content[result['metadata_storage_path']]=result['pages']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5",
      "metadata": {},
      "source": [
        "## Comments on Query results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4",
      "metadata": {},
      "source": [
        "As seen above the semantic search feature of Azure Cognitive Search service is pretty good. It gives us the top answers and also the top results with the corresponding file and the paragraph where the answers is possible located\n",
        "Let's see if we can make this better with Azure OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec",
      "metadata": {},
      "source": [
        "## Using Azure OpenAI\n",
        "\n",
        "Of course we want OpenAI to give a better answer chat style, so we instead of sending this results, we send the content of this articles to OpenAI and lets GPT model give the answer.\n",
        "\n",
        "The problem is that the content of the search result files is or can be very lengthy, more than the 4096 tokens allowed by the GPT Azure OpenAI models. So what we need to do is to split in chunks, vectorize and do a vector semantic search. \n",
        "\n",
        "Notice that **the documents chunks are already done in Azure Search**. file_content dictionary (created in the cell above) contains the pages (chunks) of each document. So we dont really need to chunk them again, each doc page for sure will fit on the max tokens limit of davinci 003 and embeddint-ada 002 models.\n",
        "\n",
        "\n",
        "We will use a genius library call LangChain that wraps a lot of boiler plate code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665",
      "metadata": {
        "gather": {
          "logged": 1679978718108
        }
      },
      "outputs": [],
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_KEY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a03f1f10-32b0-4c1e-8a0e-eee1b1d29ce7",
      "metadata": {
        "gather": {
          "logged": 1679978718287
        }
      },
      "outputs": [],
      "source": [
        "# In Azure OpenAI create a deployment for the model \"text-embedding-ada-002\"\n",
        "# and VERY IMPORTANT name the deployment the same: \"text-embedding-ada-002\"\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8f7b41d2-65b0-4058-8a46-c76cf6960720",
      "metadata": {
        "gather": {
          "logged": 1679978718468
        }
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "for key,value in file_content.items():\n",
        "    for page in value:\n",
        "        docs.append(Document(page_content=page, metadata={\"source\": key}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3315033a-4a08-4db5-8f5c-fa0a99892dc4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 146 ms, sys: 17.1 ms, total: 163 ms\n",
            "Wall time: 4.87 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if(len(docs)>1):\n",
        "    db = FAISS.from_documents(docs, embeddings)\n",
        "else:\n",
        "    print(\"No results Found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "57429335-34d3-458a-b7c9-52482a0936d5",
      "metadata": {
        "gather": {
          "logged": 1679978722930
        }
      },
      "outputs": [],
      "source": [
        "docs_db = db.similarity_search(QUESTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793c4788-715e-44dd-b2b8-3f1c3201e4e0",
      "metadata": {},
      "source": [
        "The default prompts used by Langchain for the chain_type=refine can be found here. We modified them a bit to include language option\n",
        "\n",
        "https://github.com/hwchase17/langchain/blob/master/langchain/chains/qa_with_sources/refine_prompts.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b9f65e2f-8333-4f9f-adce-fe434019e243",
      "metadata": {
        "gather": {
          "logged": 1679978723190
        }
      },
      "outputs": [],
      "source": [
        "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0.5, max_tokens=500)\n",
        "chain = load_qa_chain(llm, chain_type=\"refine\", question_prompt=REFINE_QUESTION_PROMPT, refine_prompt=REFINE_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "26a31887-0096-4c2b-94e3-4c4937920e2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 14.4 ms, sys: 362 µs, total: 14.7 ms\n",
            "Wall time: 9.33 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# TRY CHANGING THE LANGUAGE\n",
        "\n",
        "response = chain({\"input_documents\": docs_db, \"question\": QUESTION, \"language\": \"English\"}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "addffeab-527e-407e-9949-3301cad6404c",
      "metadata": {
        "gather": {
          "logged": 1679978731778
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h4>Azure OpenAI Answer:</h4>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "Markov chains are stochastic models that describe a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In Theorem 1, the equilibrium distribution of the ith state is given by πi = π0 ∞ ∑ j=i fj. Examples of Markov chains include biological processes, financial markets, and the procedure for generating the sequence {Xn : n ∈ N}, as described in Table I."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(HTML('<h4>Azure OpenAI Answer:</h4>'))\n",
        "display(HTML(response['output_text']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d3a2aba5",
      "metadata": {},
      "source": [
        "### Using Azure OpenAI (ChatGPT-Turbo) \n",
        "Now we try the same approach as before but instead of using GPT 3 we will use GPT 3.5 (ChatGPT-Turbo) model. This model is a bit more specialized in chat style answers and is also a bit smaller than GPT 3. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1610fe4f",
      "metadata": {
        "gather": {
          "logged": 1679978804543
        }
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
        "os.environ[\"OPENAI_API_TYPE\"] = os.environ[\"AZURE_OPENAI_API_TYPE\"] = \"azure\"\n",
        "\n",
        "# In Azure OpenAI create a deployment named \"gpt-35-turbo\" for the model \"gpt-35-turbo (0301)\"\n",
        "#llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", model_name=\"gpt-3.5-turbo-0301\", temperature=0.5, max_tokens=500)\n",
        "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", temperature=0.5, max_tokens=500)\n",
        "chain = load_qa_chain(llm, chain_type=\"refine\", return_refine_steps=True, question_prompt=REFINE_QUESTION_PROMPT, refine_prompt=REFINE_PROMPT)\n",
        "# chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=STUFF_PROMPT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5908d366",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 16.3 ms, sys: 3.56 ms, total: 19.9 ms\n",
            "Wall time: 10.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# TRY CHANGING THE LANGUAGE\n",
        "\n",
        "response = chain({\"input_documents\": docs_db, \"question\": QUESTION, \"language\": \"English\"}, return_only_outputs=False)\n",
        "response = chain({\"input_documents\": [Document(page_content=response['output_text'])], \"question\": QUESTION, \"language\": \"English\"}, return_only_outputs=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a0ed898f",
      "metadata": {
        "gather": {
          "logged": 1679978820407
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h4>Azure OpenAI (ChatGPT-Turbo) Answer:</h4>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Markov chains are a mathematical model that describes a sequence of events where the probability of each event depends only on the state attained in the previous event. They can be used to generate a time series with long-range dependence and a given mean and Hurst parameter. Markov chains are also used in real-world applications such as natural language processing, where they can be used to model the probability of a word given the previous words in a sentence."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(HTML('<h4>Azure OpenAI (ChatGPT-Turbo) Answer:</h4>'))\n",
        "display(HTML(response['output_text']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0",
      "metadata": {},
      "source": [
        "##### This answer is way better than taking just the result from Azure Cognitive Search. So the summary is:\n",
        "- Azure Cognitive Search give us the top results (context)\n",
        "- Azure OpenAI takes these results and understand the content and uses it as context to give the best answer\n",
        "- Best of two worlds!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e5a404-923f-4139-9cce-3b8aeee163f0",
      "metadata": {},
      "source": [
        "# Difference of ChatGPT vs GPT Smart Search results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4583036-95ab-43ed-a70f-1e96d83a079d",
      "metadata": {},
      "source": [
        "We are using the language power of GPT-3 trained models in OpenAI to understand our questions and to respond accordingly but only within the context of our data.\n",
        "Try for yourself asking the same question in ChatGPT vs here"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
