{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Queries with and without Azure OpenAI"
      ],
      "metadata": {},
      "id": "d59d527f-1100-45ff-b051-5f7c9029d94d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our Search Engine loaded and running, we are going to try some example queries and then use Azure OpenAI service to see if we can get even better results"
      ],
      "metadata": {},
      "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up variables"
      ],
      "metadata": {},
      "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "from app.embeddings import OpenAIEmbeddings\n",
        "from app.prompts import STUFF_PROMPT, REFINE_PROMPT, REFINE_QUESTION_PROMPT\n",
        "from app.credentials import (\n",
        "    API_VERSION,\n",
        "    DATASOURCE_CONNECTION_STRING,\n",
        "    AZURE_SEARCH_ENDPOINT,\n",
        "    AZURE_SEARCH_KEY,\n",
        "    COG_SERVICES_NAME,\n",
        "    COG_SERVICES_KEY,\n",
        "    AZURE_OPENAI_ENDPOINT,\n",
        "    AZURE_OPENAI_KEY,\n",
        "    AZURE_OPENAI_TYPE,\n",
        "    AZURE_OPENAI_API_VERSION\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1679579733537
        }
      },
      "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the Payloads header\n",
        "headers = {'Content-Type': 'application/json','api-key': AZURE_SEARCH_KEY}\n",
        "params = {'api-version': API_VERSION}"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1679579733816
        }
      },
      "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without Azure OpenAI"
      ],
      "metadata": {},
      "id": "9297d29b-1f61-4dce-858e-bf4272172dba"
    },
    {
      "cell_type": "code",
      "source": [
        "# Index that we are going to query (from Notebook 01)\n",
        "index_name = \"cogsrch-index\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1679579734025
        }
      },
      "id": "5a46e2d3-298a-4708-83de-9e108b1a117a"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"How Markov chains work?\" \n",
        "\n",
        "# Try questions that you think might be answered or addressed in computer science papers in 2020-2021\n",
        "# And compare the results with the open version of ChatGPT\n",
        "# The idea is that the answers using Azure OpenAI only looks at the information contained on these publications.\n",
        "\n",
        "# For Example:\n",
        "# What is CLP?\n",
        "# How Markov chains work?\n",
        "# What are some examples of reinforcement learning?"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1679580156954
        }
      },
      "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead"
    },
    {
      "cell_type": "code",
      "source": [
        "url = AZURE_SEARCH_ENDPOINT + '/indexes/'+ index_name + '/docs'\n",
        "url += '?api-version={}'.format(API_VERSION)\n",
        "url += '&search={}'.format(QUESTION)\n",
        "url += '&select=pages'\n",
        "url += '&$top=5'\n",
        "url += '&queryLanguage=en-us'\n",
        "url += '&queryType=semantic'\n",
        "url += '&semanticConfiguration=my-semantic-config'\n",
        "url += '&$count=true'\n",
        "url += '&speller=lexicon'\n",
        "url += '&answers=extractive|count-3'\n",
        "url += '&captions=extractive|highlight-true'\n",
        "url += '&highlightPreTag=' + urllib.parse.quote('<span style=\"background-color: #f5e8a3\">', safe='')\n",
        "url += '&highlightPostTag=' + urllib.parse.quote('</span>', safe='')\n",
        "\n",
        "resp = requests.get(url, headers=headers)\n",
        "print(url)\n",
        "print(resp.status_code)\n",
        "\n",
        "search_results = resp.json()\n",
        "print(\"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "https://azure-cog-search-bbuqqkw6y2eee.search.windows.net/indexes/cogsrch-index/docs?api-version=2021-04-30-Preview&search=How Markov chains work?&select=pages&$top=5&queryLanguage=en-us&queryType=semantic&semanticConfiguration=my-semantic-config&$count=true&speller=lexicon&answers=extractive|count-3&captions=extractive|highlight-true&highlightPreTag=%3Cspan%20style%3D%22background-color%3A%20%23f5e8a3%22%3E&highlightPostTag=%3C%2Fspan%3E\n200\nResults Found: 9101, Results Returned: 5\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1679580157577
        }
      },
      "id": "6f5c7e9f-42b9-4081-8975-648e4b31e175"
    },
    {
      "cell_type": "code",
      "source": [
        "# Answers from semantic Search\n",
        "display(HTML('<h4>Top Answers</h4>'))\n",
        "for result in search_results['@search.answers']:\n",
        "    if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
        "        display(HTML('<h5>' + 'Answer - score: ' + str(result['score']) + '</h5>'))\n",
        "        display(HTML(result['text']))\n",
        "\n",
        "        \n",
        "# Results from key-word search\n",
        "file_content = dict()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "display(HTML('<h4>Top Results</h4>'))\n",
        "for result in search_results['value']:\n",
        "    if result['@search.rerankerScore'] > 0.4: # Show results that are at least 10% of the max possible score=4\n",
        "        display(HTML('<h5>' + result['metadata_storage_name'] + '&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: ' + str(result['@search.rerankerScore']) + '</h5>'))\n",
        "        display(HTML(result['@search.captions'][0]['text']))\n",
        "        file_content[result['metadata_storage_path']]=result['pages']"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h4>Top Answers</h4>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5>Answer - score: 0.779296875</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Markov chain Monte Carlo (MCMC) does this by constructing a Markov chain that uses the underlying distribution as its stationary distribution. This enables the simulation of the stochastic process for non-standard distributions, while ensuring that the SLLN will hold. As an illustration of the MCMC we will discuss the Metropolis algorithm [11]."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\n\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h4>Top Results</h4>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5>0403040v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.191192626953125</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Theorem 1 Let M ′ be the Markov chain defined over the space A′ of all connected  acyclic digraphs, together with the transitions defined by the rules (T ′ 1) and (T2). The chain M ′ is irreducible, that is, given two connected acyclic digraphs G and  H, there exists in M ′ a sequence of transitions G = G0 → G1, G1 → G2, . . ."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5>0603059v2.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.16461181640625</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "here the markov chain is defined by a 2×2 stochastic matrix π = [πij ] (the reader should not confuse π with the 4 × 4 matrix ∆:          π00(1 − ε) π00ε π01(1 − ε) π01ε π00(1 − ε) π00ε π01(1 − ε) π01ε π10(1 − ε) π10ε π11(1 − ε) π11ε π10(1 − ε) π10ε π11(1 − ε) π11ε          ,  which defines the hidden markov chain via a deterministic function) …"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5>0603059v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.145233154296875</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "A function Z = {Z∞ −∞} of the Markov chain Y with the form Z = Φ(Y )  is called a hidden Markov chain; here Φ is a finite valued function defined on {1, 2, · · · , B}, taking values in {1, 2, · · · , A}."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5>0309016v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.0449981689453125</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Markov chain Monte Carlo (MCMC) does this by constructing a Markov chain that uses the underlying distribution as its stationary distribution. This enables the simulation of the stochastic process for non-standard distributions, while ensuring that the SLLN will hold. As an illustration of the MCMC we will discuss the Metropolis algorithm [11]."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5>0703031v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 1.9712371826171875</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "One step of the Markov chain M̃  1. With probability 1/2, set X ′ = X 2. With the remaining probability  a. Choose γ ∈ F(G) u.a.r. b. If γ is directed then obtain X ′ from X by reversing the orientation  of all the edges in γ. c. Otherwise, if there is a length h tower T = (γi)1≤i≤h with γ1 = γ  then let C = ⊕  1≤i≤h Fi."
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1679580158322
        }
      },
      "id": "015f0259-6609-4032-b8ae-09e95c69cb55"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments on Query results"
      ],
      "metadata": {},
      "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above the semantic search feature of Azure Cognitive Search service is pretty good. It gives us the top answers and also the top results with the corresponding file and the paragraph where the answers is possible located\n",
        "Let's see if we can make this better with Azure OpenAI"
      ],
      "metadata": {},
      "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Azure OpenAI\n",
        "\n",
        "Of course we want OpenAI to give a better answer chat style, so we instead of sending this results, we send the content of this articles to OpenAI and lets GPT model give the answer.\n",
        "\n",
        "The problem is that the content of the search result files is or can be very lengthy, more than the 4096 tokens allowed by the GPT Azure OpenAI models. So what we need to do is to split in chunks, vectorize and do a vector semantic search. \n",
        "\n",
        "Notice that **the documents chunks are already done in Azure Search**. file_content dictionary (created in the cell above) contains the pages (chunks) of each document. So we dont really need to chunk them again, each doc page for sure will fit on the max tokens limit of davinci 003 and embeddint-ada 002 models.\n",
        "\n",
        "\n",
        "We will use a genius library call LangChain that wraps a lot of boiler plate code."
      ],
      "metadata": {},
      "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_KEY\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1679580158617
        }
      },
      "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665"
    },
    {
      "cell_type": "code",
      "source": [
        "# In Azure OpenAI create a deployment for the model \"text-embedding-ada-002\"\n",
        "# and VERY IMPORTANT name the deployment the same: \"text-embedding-ada-002\"\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1679580158918
        }
      },
      "id": "a03f1f10-32b0-4c1e-8a0e-eee1b1d29ce7"
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "for key,value in file_content.items():\n",
        "    for page in value:\n",
        "        docs.append(Document(page_content=page, metadata={\"source\": key}))"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1679580159883
        }
      },
      "id": "8f7b41d2-65b0-4058-8a46-c76cf6960720"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "if(len(docs)>1):\n",
        "    db = FAISS.from_documents(docs, embeddings)\n",
        "else:\n",
        "    print(\"No results Found\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 131 ms, sys: 4.97 ms, total: 136 ms\nWall time: 4.06 s\n"
        }
      ],
      "execution_count": 33,
      "metadata": {},
      "id": "3315033a-4a08-4db5-8f5c-fa0a99892dc4"
    },
    {
      "cell_type": "code",
      "source": [
        "docs_db = db.similarity_search(QUESTION)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1679580165894
        }
      },
      "id": "57429335-34d3-458a-b7c9-52482a0936d5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default prompts used by Langchain for the chain_type=refine can be found here. We modified them a bit to include language option\n",
        "\n",
        "https://github.com/hwchase17/langchain/blob/master/langchain/chains/qa_with_sources/refine_prompts.py"
      ],
      "metadata": {},
      "id": "793c4788-715e-44dd-b2b8-3f1c3201e4e0"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0.5, max_tokens=500)\n",
        "chain = load_qa_chain(llm, chain_type=\"refine\", question_prompt=REFINE_QUESTION_PROMPT, refine_prompt=REFINE_PROMPT)"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1679580166116
        }
      },
      "id": "b9f65e2f-8333-4f9f-adce-fe434019e243"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# TRY CHANGING THE LANGUAGE\n",
        "\n",
        "response = chain({\"input_documents\": docs_db, \"question\": QUESTION, \"language\": \"English\"}, return_only_outputs=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 16.6 ms, sys: 674 µs, total: 17.3 ms\nWall time: 13.8 s\n"
        }
      ],
      "execution_count": 36,
      "metadata": {},
      "id": "26a31887-0096-4c2b-94e3-4c4937920e2e"
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML('<h4>Azure OpenAI Answer:</h4>'))\n",
        "display(HTML(response['output_text']))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h4>Azure OpenAI Answer:</h4>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n\nMarkov chains are stochastic models used to describe the probability of transitioning from one state to another. They are defined by a 2x2 stochastic matrix, Π, which contains the probability of transitioning from one state to another. When the determinant of Π is 0, the rows of Π are identical, and the sequence is an i.i.d. random sequence with a distribution (π00, π01). The hidden Markov chain is then defined by a deterministic function, and the probability of transitioning from one state to another is determined by Π. At a Black Hole, one can calculate the derivatives of H(Z) by taking the derivatives of Hn(Z) for large enough n. Examples of how Markov chains can be used include the Wang-Swendsen-Kotecký algorithm for sampling colorings and the Markov chain algorithms used to analyze planar lattice structures."
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1679580177864
        }
      },
      "id": "addffeab-527e-407e-9949-3301cad6404c"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Using Azure OpenAI (ChatGPT-Turbo) \n",
        "Now we try the same approach as before but instead of using GPT 3 we will use GPT 3.5 (ChatGPT-Turbo) model. This model is a bit more specialized in chat style answers and is also a bit smaller than GPT 3. "
      ],
      "metadata": {},
      "id": "d3a2aba5"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2022-12-01\"\n",
        "os.environ[\"OPENAI_API_TYPE\"] = os.environ[\"AZURE_OPENAI_API_TYPE\"] = \"azure\"\n",
        "\n",
        "# In Azure OpenAI create a deployment named \"gpt-35-turbo\" for the model \"gpt-35-turbo (0301)\"\n",
        "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", model_name=\"gpt-3.5-turbo-0301\", temperature=0.5, max_tokens=500)\n",
        "chain = load_qa_chain(llm, chain_type=\"refine\", return_refine_steps=True, question_prompt=REFINE_QUESTION_PROMPT, refine_prompt=REFINE_PROMPT)\n",
        "# chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=STUFF_PROMPT)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1679580402644
        }
      },
      "id": "1610fe4f"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# TRY CHANGING THE LANGUAGE\n",
        "\n",
        "response = chain({\"input_documents\": docs_db, \"question\": QUESTION, \"language\": \"English\"}, return_only_outputs=False)\n",
        "response = chain({\"input_documents\": [Document(page_content=response['output_text'])], \"question\": QUESTION, \"language\": \"English\"}, return_only_outputs=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 27.8 ms, sys: 3.27 ms, total: 31.1 ms\nWall time: 9.48 s\n"
        }
      ],
      "execution_count": 42,
      "metadata": {},
      "id": "5908d366"
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML('<h4>Azure OpenAI (ChatGPT-Turbo) Answer:</h4>'))\n",
        "display(HTML(response['output_text']))\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h4>Azure OpenAI (ChatGPT-Turbo) Answer:</h4>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Markov chains are a type of stochastic process where the probability of each event depends only on the state of the system at the previous event. They are used to model a wide range of phenomena and have applications in fields such as physics, computer science, and finance. However, the provided context does not offer a comprehensive explanation of how Markov chains work in general."
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1679580415273
        }
      },
      "id": "a0ed898f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### This answer is way better than taking just the result from Azure Cognitive Search. So the summary is:\n",
        "- Azure Cognitive Search give us the top results (context)\n",
        "- Azure OpenAI takes these results and understand the content and uses it as context to give the best answer\n",
        "- Best of two worlds!"
      ],
      "metadata": {},
      "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference of ChatGPT vs GPT Smart Search results"
      ],
      "metadata": {},
      "id": "d1e5a404-923f-4139-9cce-3b8aeee163f0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the language power of GPT-3 trained models in OpenAI to understand our questions and to respond accordingly but only within the context of our data.\n",
        "Try for yourself asking the same question in ChatGPT vs here"
      ],
      "metadata": {},
      "id": "b4583036-95ab-43ed-a70f-1e96d83a079d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}