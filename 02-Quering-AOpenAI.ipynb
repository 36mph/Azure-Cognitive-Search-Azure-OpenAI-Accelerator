{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59d527f-1100-45ff-b051-5f7c9029d94d",
   "metadata": {},
   "source": [
    "# Queries with and without Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba",
   "metadata": {},
   "source": [
    "Now that we have our Search Engine loaded and running, we are going to try some example queries and then use Azure OpenAI service to see if we can get even better results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from IPython.display import display, HTML\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from app.embeddings import OpenAIEmbeddings\n",
    "from app.prompts import STUFF_PROMPT, REFINE_PROMPT, REFINE_QUESTION_PROMPT\n",
    "from app.credentials import (\n",
    "    API_VERSION,\n",
    "    DATASOURCE_CONNECTION_STRING,\n",
    "    AZURE_SEARCH_ENDPOINT,\n",
    "    AZURE_SEARCH_KEY,\n",
    "    COG_SERVICES_NAME,\n",
    "    COG_SERVICES_KEY,\n",
    "    AZURE_OPENAI_ENDPOINT,\n",
    "    AZURE_OPENAI_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': AZURE_SEARCH_KEY}\n",
    "params = {'api-version': API_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297d29b-1f61-4dce-858e-bf4272172dba",
   "metadata": {},
   "source": [
    "## Without Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a46e2d3-298a-4708-83de-9e108b1a117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index that we are going to query (from Notebook 01)\n",
    "index_name = \"cogsrch-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is GPT?\" # Notice that is misspelled intentionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f5c7e9f-42b9-4081-8975-648e4b31e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://azure-cog-search-bbuqqkw6y2eee.search.windows.net/indexes/cogsrch-index/docs?api-version=2021-04-30-Preview&search=What is GPT?&select=pages&$top=5&queryLanguage=en-us&queryType=semantic&semanticConfiguration=my-semantic-config&$count=true&speller=lexicon&answers=extractive|count-3&captions=extractive|highlight-true&highlightPreTag=%3Cspan%20style%3D%22background-color%3A%20%23f5e8a3%22%3E&highlightPostTag=%3C%2Fspan%3E\n",
      "200\n",
      "Results Found: 9787, Results Returned: 5\n"
     ]
    }
   ],
   "source": [
    "url = AZURE_SEARCH_ENDPOINT + '/indexes/'+ index_name + '/docs'\n",
    "url += '?api-version={}'.format(API_VERSION)\n",
    "url += '&search={}'.format(QUESTION)\n",
    "url += '&select=pages'\n",
    "url += '&$top=5'\n",
    "url += '&queryLanguage=en-us'\n",
    "url += '&queryType=semantic'\n",
    "url += '&semanticConfiguration=my-semantic-config'\n",
    "url += '&$count=true'\n",
    "url += '&speller=lexicon'\n",
    "url += '&answers=extractive|count-3'\n",
    "url += '&captions=extractive|highlight-true'\n",
    "url += '&highlightPreTag=' + urllib.parse.quote('<span style=\"background-color: #f5e8a3\">', safe='')\n",
    "url += '&highlightPostTag=' + urllib.parse.quote('</span>', safe='')\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "print(url)\n",
    "print(resp.status_code)\n",
    "\n",
    "search_results = resp.json()\n",
    "print(\"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "015f0259-6609-4032-b8ae-09e95c69cb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Top Answers</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>Answer - score: 0.9658203125</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We design a generalized Paige-Tarjan refinement algorithm, called GPT, that, for any abstract model A ∈ A, computes the most abstract refinement of A in A which is strongly preserving for the language LOp . The correctness of GPT is guaranteed by some completeness conditions on A and Op."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>Answer - score: 0.9658203125</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We design a generalized Paige-Tarjan refinement algorithm, called GPT, that, for any abstract model A ∈ A, computes the most abstract refinement of A in A which is strongly preserving for the language LOp . The correctness of GPT is guaranteed by some completeness conditions on A and Op."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>Answer - score: 0.744140625</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This generalized pignistic transformation (GPT) is defined by: ∀A ∈ DΘ,  P{A} = ∑  X∈DΘ  CM(X ∩ A)  CM(X) m(X) (9)  where CM(X) denotes the DSm cardinal of proposition X for the DSm model M of the problem under consideration. The decision about the solution of the problem is usually taken by the maximum of pignistic probability function P{.}."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>Top Results</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>0409007v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.09490966796875</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This generalized pignistic transformation (GPT) is defined by: ∀A ∈ DΘ,  P{A} = ∑  X∈DΘ  CM(X ∩ A)  CM(X) m(X) (9)  where CM(X) denotes the DSm cardinal of proposition X for the DSm model M of the problem under consideration. The decision about the solution of the problem is usually taken by the maximum of pignistic probability function P{.}."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>0412091v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 1.90692138671875</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "the generalized pignistic transformation (gpt) is presented in next section.  2.8 the generalized pignistic transformation (gpt)  2.8.1 the classical pignistic transformation  we follow here the smets’ vision which considers the management of information as a two 2-levels process: credal (for combination of evidences) and pignistic7 (for …"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>0608002v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 1.674591064453125</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "the generalized pignistic transformation (gpt) is presented in next section.  3 proportional conflict redistribution rule  instead of applying a direct transfer of partial conflicts onto partial uncertainties as with dsmh, the idea behind the proportional conflict redistribution (pcr) rule [25, 26] is to transfer (total or partial) conflicting …"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>0612120v1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 1.53106689453125</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We design a generalized Paige-Tarjan refinement algorithm, called GPT, that, for any abstract model A ∈ A, computes the most abstract refinement of A in A which is strongly preserving for the language LOp . The correctness of GPT is guaranteed by some completeness conditions on A and Op."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>0612120v2.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 1.527923583984375</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We design a generalized Paige-Tarjan refinement algorithm, called GPT, that, for any abstract model A ∈ A, computes the most abstract refinement of A in A which is strongly preserving for the language LOp . The correctness of GPT is guaranteed by some completeness conditions on A and Op."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answers from semantic Search\n",
    "display(HTML('<h4>Top Answers</h4>'))\n",
    "for result in search_results['@search.answers']:\n",
    "    if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
    "        display(HTML('<h5>' + 'Answer - score: ' + str(result['score']) + '</h5>'))\n",
    "        display(HTML(result['text']))\n",
    "\n",
    "        \n",
    "# Results from key-word search\n",
    "file_content = dict()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "display(HTML('<h4>Top Results</h4>'))\n",
    "for result in search_results['value']:\n",
    "    if result['@search.rerankerScore'] > 0.4: # Show results that are at least 10% of the max possible score=4\n",
    "        display(HTML('<h5>' + result['metadata_storage_name'] + '&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: ' + str(result['@search.rerankerScore']) + '</h5>'))\n",
    "        display(HTML(result['@search.captions'][0]['text']))\n",
    "        file_content[result['metadata_storage_path']]=result['pages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5",
   "metadata": {},
   "source": [
    "## Comments on Query results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4",
   "metadata": {},
   "source": [
    "As seen above the semantic search feature of Azure Cognitive Search service is pretty good. It gives us the top answers and also the top results with the corresponding file and the paragraph where the answers is possible located\n",
    "Let's see if we can make this better with Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec",
   "metadata": {},
   "source": [
    "## Using Azure OpenAI\n",
    "\n",
    "Of course we want OpenAI to give a better answer chat style, so we instead of sending this results, we send the content of this articles to OpenAI and lets GPT model give the answer.\n",
    "\n",
    "The problem is that the content of the search result files is or can be very lengthy, more than the 4096 tokens allowed by the GPT Azure OpenAI models. So what we need to do is to split in chunks, vectorize and do a vector semantic search. \n",
    "Let's do that\n",
    "\n",
    "We will use a genius library call LangChain that wraps a lot of boiler plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a03f1f10-32b0-4c1e-8a0e-eee1b1d29ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Azure OpenAI create a deployment for the model \"text-embedding-ada-002\"\n",
    "# and VERY IMPORTANT name the deployment the same: \"text-embedding-ada-002\"\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f7b41d2-65b0-4058-8a46-c76cf6960720",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for key,value in file_content.items():\n",
    "    for page in value:\n",
    "        docs.append(Document(page_content=page, metadata={\"source\": key}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3315033a-4a08-4db5-8f5c-fa0a99892dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 260 ms, sys: 15.3 ms, total: 275 ms\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if(len(docs)>1):\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "else:\n",
    "    print(\"No results Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57429335-34d3-458a-b7c9-52482a0936d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_db = db.similarity_search(QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c4788-715e-44dd-b2b8-3f1c3201e4e0",
   "metadata": {},
   "source": [
    "The default prompts used by Langchain for the chain_type=refine can be found here. We modified them a bit to include language and summarization\n",
    "\n",
    "https://github.com/hwchase17/langchain/blob/master/langchain/chains/qa_with_sources/refine_prompts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9f65e2f-8333-4f9f-adce-fe434019e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0.5, max_tokens=500)\n",
    "chain = load_qa_chain(llm, chain_type=\"refine\", question_prompt=REFINE_QUESTION_PROMPT, refine_prompt=REFINE_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26a31887-0096-4c2b-94e3-4c4937920e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 ms, sys: 2.04 ms, total: 15.4 ms\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRY CHANGING THE LANGUAGE\n",
    "\n",
    "response = chain({\"input_documents\": docs_db, \"question\": QUESTION, \"language\": \"English\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "addffeab-527e-407e-9949-3301cad6404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Azure OpenAI Answer:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "GPT (Generalized Partition Refinement) is an algorithm used to compute the coarsest strongly preserving partition of a given language. It is used to compute the coarsest strongly preserving partition PL for languages that are closed under conjunction and negation such as CTL-X/CTL∗-X, and the partition Pdbs, the largest divergence blind stuttering equivalence relation. Additionally, the Groote-Vaandrager algorithm is an optimized instance of GPT used to efficiently compute the partition Pdbs. GPT is also used to compute branching bisimulation equivalence, simulation equivalence, and strong preservation with respect to the language inductively generated by propositional logic and the reachability operator EF. It is bounded by the number of states and takes O(|Σ|max({cost(f) | f ∈ F})) time, and can be implemented with space and time complexities that are competitive with the best available algorithms for simulation equivalence."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h4>Azure OpenAI Answer:</h4>'))\n",
    "display(HTML(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0",
   "metadata": {},
   "source": [
    "##### This answer is way better than taking just the result from Azure Cognitive Search. So the summary is:\n",
    "- Azure Cognitive Search give us the top results\n",
    "- Azure OpenAI construct takes this results and understand them to give the best answer\n",
    "- Best of two worlds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5a404-923f-4139-9cce-3b8aeee163f0",
   "metadata": {},
   "source": [
    "# Difference of ChatGPT vs GPT Smart Search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4583036-95ab-43ed-a70f-1e96d83a079d",
   "metadata": {},
   "source": [
    "We are using the language power of GPT-3 trained models in OpenAI to understand our questions and to respond accordingly but only within the context of our data.\n",
    "Try for yourself asking the same question in ChatGPT vs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f5bd8-0d56-47b8-84fd-fe9b678bfc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
