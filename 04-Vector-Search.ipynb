{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec6048-44e4-4118-b16a-9c4c9cc78a3b",
   "metadata": {},
   "source": [
    "# Azure Vector Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281ac79-47cd-49d4-bdd4-7f5c173a947d",
   "metadata": {},
   "source": [
    "**Azure Cognitive Search has now vector search capabilities** ([Watch this video](https://aka.ms/Vector_SearchSnackableVideo)). The advantages of vector search in Azure Cognitive Search include its integration with other capabilities of Azure Cognitive Search, the ability to use any type of data (text, image, audio, video, etc) from diverse Azure datastores to inform a single generative AI-powered application, and the support of vector fields in the search indexes. It also offers pure vector search, hybrid retrieval, and a sophisticated re-ranking system powered by Bing in a single integrated solution (check the release [blog site](https://techcommunity.microsoft.com/t5/azure-ai-services-blog/announcing-vector-search-in-azure-cognitive-search-public/ba-p/3872868)).\n",
    "\n",
    "\n",
    "![vector-search](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/489211i001E2B9B34F483C2/image-dimensions/876x416?v=v2)\n",
    "\n",
    "\n",
    "**The main limitations (for now) of vector search in Azure Cognitive Search are:**\n",
    "\n",
    "- It does not generate vector embeddings for the content. Users need to provide the embeddings themselves by using a service such as Azure OpenAI.\n",
    "- There is not field type for Collection of vectors, meaning that each document in the vector-based index must be either a small document or a chunk of a bigger document.\n",
    "\n",
    "<br>\n",
    "So, based on the information above, more questions arise:\n",
    "\n",
    "1) **How do we create the vectors of each document in the index? do we need to manually split the text, vectorize the chunk and push it to a new vector-based index?**\n",
    "2) **Or, can we use the existing text-based-ai-enriched index that can ingest any type of file on a schedule, and use it as a base for a new vector-based index?**\n",
    "\n",
    "The answer, as usual, is: it depends.\n",
    "\n",
    "Let's think about this, if your use case is just PDFs, for example, you can just use [PyPDF library](https://pypi.org/project/pypdf/) or [Azure AI Document Intelligence SDK (former Form Recognizer)](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-3.0.0), vectorize using OpenAI API and push the content to the vector-based index. And this is problably the simplest and fastest way to go.  However if your use case entails connecting to a datalake, or Sharepoint libraries or any other document data source with thousands of documents with multiple file types and that can change dynamically, then you would want to use the Ingestion and Document Cracking and AI-Enrichment capabilities of Azure Search engine and avoid a lot of painful custom code. \n",
    "\n",
    "Let's try both:\n",
    "\n",
    "1. Manually parse PDFs documents using pypdf library and Azure AI Document Intelligence, create the chunks, vectorize each chunk, and push the chunk vector and chunk text to a vector-based index.\n",
    "2. Use our current text-based indexes that has already chunks on it, vectorize each chunk using a custom skill function or on-demand (as documents are discovered by user searches), load new vector-based indexes, and use these new indexes in each user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f6044e-463f-4988-bc46-a3c3d641c15c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'read_pdf_files' from 'common.utils' (/mnt/batch/tasks/shared/LS_root/mounts/clusters/pabmar2/code/Users/pabmar/GPT-Azure-Search-Engine/common/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_qa_chain\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqa_with_sources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_qa_with_sources_chain\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_pdf, read_pdf_files, text_to_base64\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMBINE_QUESTION_PROMPT, COMBINE_PROMPT\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_tokens_limit, num_tokens_from_docs\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'read_pdf_files' from 'common.utils' (/mnt/batch/tasks/shared/LS_root/mounts/clusters/pabmar2/code/Users/pabmar/GPT-Azure-Search-Engine/common/utils.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "from common.utils import parse_pdf, read_pdf_files, text_to_base64\n",
    "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT\n",
    "from common.utils import model_tokens_limit, num_tokens_from_docs\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "os.makedirs(\"data/books/\",exist_ok=True)\n",
    "    \n",
    "# Set the Data source connection string.\n",
    "# You can change it and use your own data if you wish\n",
    "BLOB_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=demodatasetsp;AccountKey=QVFgIKPiWB+8f0mH+F7fidVLG7wq1S3WhtAqXOWaMWtr6fZ4frhVgmUzgBSdkmw4VsjoEAo7C2Hn+ASt2Cc5HA==;EndpointSuffix=core.windows.net\"\n",
    "BLOB_SAS_TOKEN=\"?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\"\n",
    "BLOB_CONTAINER_NAME = \"books\"\n",
    "BASE_CONTAINER_URL = \"https://demodatasetsp.blob.core.windows.net/\" + BLOB_CONTAINER_NAME + \"/\"\n",
    "LOCAL_FOLDER = \"./data/books/\"\n",
    "\n",
    "os.makedirs(LOCAL_FOLDER,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331692ba-b68e-4b99-9bae-5057da9a389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594ff0d4-56e3-4bed-843d-28c7a092069b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedder \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87c647-158c-4f85-b569-5b9462f06c83",
   "metadata": {},
   "source": [
    "## 1 - Manual Document Cracking with Push to Vector-based Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551868-1546-421b-a14e-e42618d88e61",
   "metadata": {},
   "source": [
    "In the previous notebook, we developed solutions for various types of files and data formats commonly found in organizations. However, we encountered an issue when dealing with questions that require answers from complex files. The complexity of these files arises from their length and the way information is distributed within them.\n",
    "\n",
    "One example of such complex files is the Technical Specification Guides, which can span hundreds of pages and contain information in the form of images, tables, forms, and more. Books are also complex due to their length and the presence of images or tables.\n",
    "\n",
    "These files are typically in PDF format. To better handle these PDFs, we need a smarter parsing method that treats each document as a special source and processes them page by page. The objective is to obtain more accurate and faster answers from our system. Fortunately, there are usually not many of these types of documents in an organization, allowing us to make exceptions and treat them differently.\n",
    "\n",
    "Within our demo storage account, we have a container named `books`, which holds 5 books of different lengths, languages, and complexities. Let's create a `cogsrch-index-books-vector` and load it with the pages of all these books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0999e24b-6a75-4fa1-9a5f-426cf0f0bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = [\"Azure_Cognitive_Search_Documentation.pdf\", \n",
    "         \"Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf\",\n",
    "         \"Fundamentals_of_Physics_Textbook.pdf\",\n",
    "         \"Made_To_Stick.pdf\",\n",
    "         \"Pere_Riche_Pere_Pauvre.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd867b2f-b5a1-443c-aa0a-ce914a66b3c9",
   "metadata": {},
   "source": [
    "Let's download the files to the local `./data/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3554f0b7-fee8-4446-a155-5d22dc0f0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for book in tqdm(books):\n",
    "    book_url = BASE_CONTAINER_URL + book + BLOB_SAS_TOKEN\n",
    "    urllib.request.urlretrieve(book_url, LOCAL_FOLDER+ book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cc0db-9dae-45f2-8943-2b6fa32fcc75",
   "metadata": {},
   "source": [
    "### What to use: pyPDF or AI Documment Intelligence API (Form Recognizer)?\n",
    "\n",
    "In `utils.py` there is a **parse_pdf()** function. This utility function can parse local files using PyPDF library and can also parse local or from_url PDFs files using Azure AI Document Intelligence (Former Form Recognizer).\n",
    "\n",
    "If `form_recognizer=False`, the function will parse the PDF using the python pyPDF library, which 75% of the time does a good job.<br>\n",
    "\n",
    "Setting `form_recognizer=True`, is the best (and slower) parsing method using AI Documment Intelligence API (former known as Form Recognizer). You can specify the prebuilt model to use, the default is `model=\"prebuilt-document\"`. However, if you have a complex document with tables, charts and figures , you can try\n",
    "`model=\"prebuilt-layout\"`, and it will capture all of the nuances of each page (it takes longer of course).\n",
    "\n",
    "**Note: Many PDFs are scanned images. For example, any signed contract that was scanned and saved as PDF will NOT be parsed by pyPDF. Only AI Documment Intelligence API will work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c63a2f-7a53-4346-8a1f-483cfd159d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Text from Azure_Cognitive_Search_Documentation.pdf ...\n",
      "Extracting text using PyPDF\n",
      "Parsing took: 43.208935 seconds\n",
      "Azure_Cognitive_Search_Documentation.pdf contained 1947 pages\n",
      "\n",
      "Extracting Text from Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf ...\n",
      "Extracting text using PyPDF\n",
      "Parsing took: 2.035961 seconds\n",
      "Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf contained 357 pages\n",
      "\n",
      "Extracting Text from Fundamentals_of_Physics_Textbook.pdf ...\n",
      "Extracting text using PyPDF\n",
      "Parsing took: 128.467710 seconds\n",
      "Fundamentals_of_Physics_Textbook.pdf contained 1450 pages\n",
      "\n",
      "Extracting Text from Made_To_Stick.pdf ...\n",
      "Extracting text using PyPDF\n",
      "Parsing took: 9.256242 seconds\n",
      "Made_To_Stick.pdf contained 225 pages\n",
      "\n",
      "Extracting Text from Pere_Riche_Pere_Pauvre.pdf ...\n",
      "Extracting text using PyPDF\n",
      "Parsing took: 1.204234 seconds\n",
      "Pere_Riche_Pere_Pauvre.pdf contained 225 pages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_pages_map = dict()\n",
    "for book in books:\n",
    "    print(\"Extracting Text from\",book,\"...\")\n",
    "    \n",
    "    # Capture the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parse the PDF\n",
    "    book_path = LOCAL_FOLDER+book\n",
    "    book_map = parse_pdf(file=book_path, form_recognizer=False, verbose=True)\n",
    "    book_pages_map[book]= book_map\n",
    "    \n",
    "    # Capture the end time and Calculate the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Parsing took: {elapsed_time:.6f} seconds\")\n",
    "    print(f\"{book} contained {len(book_map)} pages\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0a722-ae0c-4b57-802a-518f5d4d93fd",
   "metadata": {},
   "source": [
    "Now let's check a random page of each book to make sure the parsing was done correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a5d62f-b664-4662-a6c9-a1eb2a3c5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure_Cognitive_Search_Documentation.pdf \n",
      " chunk text: 1. Select Create demo app  at the bottom of the page to generate the HTML file.\n",
      " ...\n",
      "\n",
      "Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf \n",
      " chunk text: 26\n",
      "father said, “Did I hear you right? You don’t think he has a\n",
      "problem?”\n",
      "“That’ ...\n",
      "\n",
      "Fundamentals_of_Physics_Textbook.pdf \n",
      " chunk text: 9PROBLEMS••6You can easily convert common units and measures electroni-cally, bu ...\n",
      "\n",
      "Made_To_Stick.pdf \n",
      " chunk text: to a halt, ongoing activities are interrupted, our attention focuses in- \n",
      "volunt ...\n",
      "\n",
      "Pere_Riche_Pere_Pauvre.pdf \n",
      " chunk text: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~\n",
      "~ ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bookname,bookmap in book_pages_map.items():\n",
    "    print(bookname,\"\\n\",\"chunk text:\",bookmap[random.randint(10, 50)][2][:80],\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdc1ee-71fc-49d2-8e7c-0964bc3a4370",
   "metadata": {},
   "source": [
    "As we can see above, all books were parsed except `Pere_Riche_Pere_Pauvre.pdf` (this book is \"Rich Dad, Poor Dad\" written in French), why? Well, as we mentioned above, this book was scanned, so each page is an image. We need a PDF parser with good OCR capabilities in order to extract the content of this PDF. \n",
    "Let's try to parse this book again, but this time using Azure Document Intelligence API (former Form Recognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "801c6bc2-467c-4418-aa7e-ef89a1e20e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text using Azure Document Intelligence\n",
      "CPU times: user 13.8 s, sys: 252 ms, total: 14.1 s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "book = \"Pere_Riche_Pere_Pauvre.pdf\"\n",
    "book_path = LOCAL_FOLDER+book\n",
    "book_map = parse_pdf(file=book_path, form_recognizer=True, model=\"prebuilt-document\",from_url=False, verbose=True)\n",
    "book_pages_map[book]= book_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97f9c5bb-c44b-4a4d-9780-591f9f8d128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pere_Riche_Pere_Pauvre.pdf \n",
      " chunk text: monde qui les attend, un univers axé davantage sur les dépenses que sur l'épargn ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(book,\"\\n\",\"chunk text:\",book_map[random.randint(10, 50)][2][:80],\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c279dfb-4fed-41b8-89e1-0ca2cefbcdc9",
   "metadata": {},
   "source": [
    "As demonstrated above, Azure Document Intelligence proves to be superior to pyPDF. For production scenarios, we strongly recommend using Azure Document Intelligence consistently. When doing so, it's important to make a wise choice between the available models, such as \"prebuilt-document,\" \"prebuilt-layout,\" or others. You can find more information on model selection [HERE](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/choose-model-feature?view=doc-intel-3.0.0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9b7d-99e6-426d-a47e-343c7e8b492e",
   "metadata": {},
   "source": [
    "## Create Vector-based index\n",
    "\n",
    "\n",
    "Now that we have the content of the book's chunks (each page of each book) in the dictionary `book_pages_map`, let's create the Vector-based index in our Azure Search Engine where this content is going to land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d46e7c5-49c4-40f3-bb2d-79a9afeab4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"cogsrch-index-books-vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b07e84b-d306-4bc9-9124-e64f252dd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Azure Search Vector-based Index\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2df4db6b-969b-4b91-963f-9334e17a4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
    "        {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunks\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunkVector\",\"type\": \"Collection(Edm.Single)\",\"searchable\": \"true\",\"retrievable\": \"true\",\"dimensions\": 1536,\"vectorSearchConfiguration\": \"vectorConfig\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"page_num\",\"type\": \"Edm.Int32\",\"searchable\": \"false\",\"retrievable\": \"true\"},\n",
    "        \n",
    "    ],\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithmConfigurations\": [\n",
    "            {\n",
    "                \"name\": \"vectorConfig\",\n",
    "                \"kind\": \"hnsw\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunks\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36691ff0-c4c8-49d0-bfa8-3e076ece0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to debug errors\n",
    "# r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7dda9-4725-410e-9465-54f0298fc758",
   "metadata": {},
   "source": [
    "## Upload the Document chunks and its vectors to the Vector-Based Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5c8aa55-1b60-4057-93db-0d4a89993a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Azure_Cognitive_Search_Documentation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1947/1947 [05:22<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 357/357 [01:00<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Fundamentals_of_Physics_Textbook.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450/1450 [04:29<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Made_To_Stick.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:39<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Pere_Riche_Pere_Pauvre.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:39<00:00,  5.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for bookname,bookmap in book_pages_map.items():\n",
    "    print(\"Uploading chunks from\",bookname)\n",
    "    for page in tqdm(bookmap):\n",
    "        try:\n",
    "            page_num = page[0] + 1\n",
    "            content = page[2]\n",
    "            book_url = BASE_CONTAINER_URL + bookname + os.environ['BLOB_SAS_TOKEN']\n",
    "            upload_payload = {\n",
    "                \"value\": [\n",
    "                    {\n",
    "                        \"id\": text_to_base64(bookname + str(page_num)),\n",
    "                        \"title\": f\"{bookname}_page_{str(page_num)}\",\n",
    "                        \"chunks\": content,\n",
    "                        \"chunkVector\": embedder.embed_query(content if content!=\"\" else \"-------\"),\n",
    "                        \"name\": bookname,\n",
    "                        \"location\": book_url,\n",
    "                        \"page_num\": page_num,\n",
    "                        \"@search.action\": \"upload\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/index\",\n",
    "                                 data=json.dumps(upload_payload), headers=headers, params=params)\n",
    "            if r.status_code != 200:\n",
    "                print(r.status_code)\n",
    "                print(r.text)\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\",e)\n",
    "            print(content)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cddcf-af7b-4006-a047-853fc7a66be3",
   "metadata": {},
   "source": [
    "## Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b408798-5527-44ca-9dba-cad2ee726aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"what normally rich dad do that is different from poor dad?\"\n",
    "# QUESTION = \"Tell me a summary of the book Boundaries\"\n",
    "# QUESTION = \"Dime que significa la radiacion del cuerpo negro\"\n",
    "# QUESTION = \"what is the acronym of the main point of Made to Stick book\"\n",
    "# QUESTION = \"Tell me a python example of how do I push documents with vectors to an index using the python SDK?\"\n",
    "# QUESTION = \"who won the soccer championship?\" # this question should have no answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05e21f39-39cb-432b-a83f-c6b69fef72a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Returned: 5\n"
     ]
    }
   ],
   "source": [
    "search_payload = {\n",
    "    \"vectors\": [{\"value\": embedder.embed_query(QUESTION),\"fields\": \"chunkVector\",\"k\": 5}],\n",
    "    \"select\": \"title, chunks, name, location, page_num\",\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search\",\n",
    "                         data=json.dumps(search_payload), headers=headers, params=params)\n",
    "\n",
    "ordered_results = r.json()\n",
    "print(\"Results Returned: {}\".format(len(ordered_results['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75eab6bf-d889-4ae4-b5ed-5e8da9f306ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@odata.context': \"https://cog-search-arbnfv3wffx5o.search.windows.net/indexes('cogsrch-index-books-vector')/$metadata#docs(*)\",\n",
       " 'value': [{'@search.score': 0.8654169,\n",
       "   'title': 'Pere_Riche_Pere_Pauvre.pdf_page_1',\n",
       "   'chunks': \"Best-seller du New York Times\\nPère riche Père pauvre\\nVersion française de Rich Dad, Poor Dad\\nCe que les parents riches enseignent à leurs enfants à propos de l'argent afin qu'il soit à leur service\\nRobert T. Kiyosaki et Sharon L. Lechter UN MONDE DIFFÉRENT \",\n",
       "   'name': 'Pere_Riche_Pere_Pauvre.pdf',\n",
       "   'location': 'https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D',\n",
       "   'page_num': 1},\n",
       "  {'@search.score': 0.85955626,\n",
       "   'title': 'Pere_Riche_Pere_Pauvre.pdf_page_3',\n",
       "   'chunks': 'Données de catalogage avant publication (Canada)\\nKiyosaki, Robert T., 1947-\\nPère riche, père pauvre : devenir riche ne s\\'apprend pas à l\\'école : ce que les parents riches enseignent à leurs enfants à propos de l\\'argent afin qu\\'il soit à leur service (Collection Réussite financière) Traduction de : Rich dad, poor dad Comprend des réf. bibliographiques\\nISBN 2-89225-447-7\\n1. Finances personnelles. 2. Investissements. 3. Riches. I. Lechter, Sharon L. II. Titre. III. Collection, HG 179.K59514 2000 332.024 C00-941627-7\\nCet ouvrage a été publié en langue anglaise sous le titre original : RICH DAD, POOR DAD, WHAT THE RECII TEACH THEIR KIDS ABOUT MONEY - THAT THEE POOR AND MIDDLE CLASS DO NOT ! Published by TechPress, Inc. 6611 N. 64\" Place Paradise Valley, Arizona 85253 U.S.A. 602-998-6971 CASHFLOW is the trademark of Cashflow Technologies, Inc.\\nCopyright @ 1997, 1998 by Robert T. Kiyosaki and Sharon L. Lechter All rights reserved\\nC. Les éditions Un monde différent ltée, 2000 Pour l\\'édition en langue française\\nDépôts légaux : 3º trimestre 2000 Bibliothèque nationale du Québec Bibliothèque nationale du Canada Bibliothèque nationale de France\\nConception graphique de la couverture, photocomposition et mise en pages : OLIVIER LASSER + JEAN-PHILIPPE GAUDET\\nVersion française : JEAN-PIERRE MANSEAL\\nISBN 978-2-89225-447-1 (Édition originale : ISBN 0-9643856-1-9 Pbk, TechPress, Inc., Arizona)\\nNous reconnaissons l\\'aide financière du gouvernement du Canada par l\\'entremise du Pro- gramme d\\'Aide au Développement de l\\'industrie de l\\'Édition pour nos activités d\\'édition (PADIÉ).\\nImprimé au Canada ',\n",
       "   'name': 'Pere_Riche_Pere_Pauvre.pdf',\n",
       "   'location': 'https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D',\n",
       "   'page_num': 3},\n",
       "  {'@search.score': 0.8583758,\n",
       "   'title': 'Pere_Riche_Pere_Pauvre.pdf_page_21',\n",
       "   'chunks': \"Chapitre un Père riche, père pauvre Tel que raconté par Robert Kiyosaki\\n'ai eu deux pères, l'un riche et l'autre pauvre. L'un était très instruit et très J intelligent; il était titulaire d'un doctorat et avait complété quatre années d'études postdoctorales en moins de deux ans. Il fréquenta ensuite l'université Stanford, l'université de Chicago et l'université Northwestern pour y faire d'autres études supérieures entièrement défrayées grâce aux bourses qu'il mérita. L'autre père ne termina même pas sa huitième année.\\nLes deux hommes eurent du succès dans leur carrière, travaillant dur toute leur vie. Tous deux gagnèrent des revenus substantiels. Et pourtant, l'un d'eux éprouva des difficultés financières pendant toute son existence. L'autre devint l'un des hommes les plus riches d'Hawaï. L'un mourut léguant des dizaines de millions de dollars à sa famille, aux organismes de charité et à son église. L'autre laissa des dettes à rembourser.\\nCes deux hommes étaient solides, charismatiques et influents. Tous deux me prodiguèrent des conseils mais pas sur les mêmes sujets. Les deux hommes croyaient fermement dans l'enseignement mais ils ne me recommandèrent pas le même programme d'études.\\n25 \",\n",
       "   'name': 'Pere_Riche_Pere_Pauvre.pdf',\n",
       "   'location': 'https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D',\n",
       "   'page_num': 21},\n",
       "  {'@search.score': 0.84728974,\n",
       "   'title': 'Pere_Riche_Pere_Pauvre.pdf_page_225',\n",
       "   'chunks': 'Robert T. Kiyosaki - Sharon L. Lechter Père riche, Père pauvre\\nDEVENIR RICHE NE S\\'APPREND PAS À L\\'ÉCOLE !\\nBest-seller américain recensé dans Business Week, The Wall Street Journal. The New York Times et USA Today\\nÉtant donné que j\\'ai eu deux pères qui m\\'ont influencé, j\\'ai appris de l\\'un et l\\'autre. Je devais réfléchir aux conseils de chaque père, et ce faisant, cela m\\'a permis de comprendre davantage le pouvoir et l\\'effet de nos propres pensées sur notre vie. Par exemple, l\\'un de mes pères avait l\\'habitude de dire: \"Je ne peux pas me permettre d\\'acheter cela.\" L\\'autre père refusait que j\\'emploie de tels mots. Il m\\'incitait plutôt à dire: \"Comment puis-je me permettre d\\'acheter cela ?\" L\\'une de ces phrases est une affirmation et l\\'autre une question. L\\'une ne vous oblige à rien et l\\'autre vous oblige à réfléchir.\\nLa vie vous a offert deux magnifiques présents : votre esprit et votre temps. Il n\\'en tient qu\\'à vous de faire ce qui vous plaît avec l\\'un et l\\'autre. Grâce à chaque billet d\\'un dollar qui passe par vos mains, vous seul avez le pouvoir de décider de votre destinée. Si vous dépensez cet argent bêtement, vous choisissez de vous appauvrir. Si vous l\\'employez à faire face à vos engagements, vous vous joignez alors à la classe moyenne. Investissez-le dans votre esprit et vous apprendrez comment acquérir des avoirs. Chaque jour, avec chaque dollar, vous déciderez d\\'être riche, pauvre ou de faire partie de la classe moyenne.\\nL\\'argent est une forme de pouvoir. Mais l\\'éducation financière est plus puissante encore. Choisissez de partager ces connaissances avec vos enfants et vous choisirez alors de les préparer pour ce monde qui les attend.\\nPrésenté en six leçons simples, l\\'auteur vous propose un parallèle entre la mentalité d\\'un père riche et celle d\\'un père pauvre. Ces leçons vous aideront à vous enrichir quoi qu\\'il advienne dans ce monde où l\\'incertitude et les changements s\\'accentuent.\\n-J\\'aime mes enfants et je veux m\\'assurer qu\\'ils puissent obtenir la meilleure éducation possible ! L\\'école traditionnelle, bien que très importante, ne suffit plus. Nous avons besoin de comprendre l\\'argent et son fonctionnement puisque chaque jour, avec chaque dollar qui passe entre nos mains, nous avons le pouvoir de décider de notre destinée.\" Suos L. LECHTER, COUTURE\\n- La principale raison pour laquelle les gens sont aux prises avec des problèmes financiers est qu\\'ils ont passé plusieurs années à l\\'école mais n\\'ont rien appris en ce qui concerne l\\'argent. Il en résulte que les gens apprennent à travailler an service de l\\'argent ... mais n\\'apprennent jamais à mettre l\\'argent à leur service. - Romer T. KIYOSikt, AUTECH\\nwww.umd.ca\\nISBN 978-2-89225-447-1\\nFINANCIERE\\ninfo@umd.ca\\n9 782892 254471 ',\n",
       "   'name': 'Pere_Riche_Pere_Pauvre.pdf',\n",
       "   'location': 'https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D',\n",
       "   'page_num': 225},\n",
       "  {'@search.score': 0.84546506,\n",
       "   'title': 'Pere_Riche_Pere_Pauvre.pdf_page_2',\n",
       "   'chunks': \"Robert T. Kiyosaki Sharon L. Lechter\\nPère riche Père pauvre\\nDEVENIR RICHE NE S'APPREND PAS À L'ÉCOLE !\\nLes éditions Un monde différent ltée 3905, rue Isabelle, bureau 101 Brossard (Québec) Canada J4Y 2R2 (450) 656-2660 Site internet : http ://www.umd.ca Courriel : info@umd.ca \",\n",
       "   'name': 'Pere_Riche_Pere_Pauvre.pdf',\n",
       "   'location': 'https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D',\n",
       "   'page_num': 2}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "410ff796-dab1-4817-a3a5-82eeff6c0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-35-turbo-16k\" # options: gpt-35-turbo, gpt-35-turbo-16k, gpt-4, gpt-4-32k\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "744aba20-b3fd-4286-8d58-2ddfccc77734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each of the results chunks and create a LangChain Document class to use further in the pipeline\n",
    "top_docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    if key == \"value\":\n",
    "        for page in value:\n",
    "            location = page[\"location\"] if page[\"location\"] is not None else \"\"\n",
    "            top_docs.append(Document(page_content=page[\"chunks\"], metadata={\"source\": location}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(top_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db1c4d56-8c2d-47d6-8717-810f156f1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom token limit for gpt-35-turbo-16k : 14500\n",
      "Combined docs tokens count: 1786\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(top_docs)>0):\n",
    "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    num_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
    "    print(\"Custom token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Combined docs tokens count:\",num_tokens)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a91c475c-7b7e-44c2-8783-d91ae05a04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain Type selected: stuff\n"
     ]
    }
   ],
   "source": [
    "chain_type = \"map_reduce\" if num_tokens > tokens_limit else \"stuff\"  \n",
    "print(\"Chain Type selected:\", chain_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62cf3a3f-2b4d-4806-8b92-eb982c52b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chain_type == \"stuff\":\n",
    "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
    "                                       prompt=COMBINE_PROMPT)\n",
    "elif chain_type == \"map_reduce\":\n",
    "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
    "                                       question_prompt=COMBINE_QUESTION_PROMPT,\n",
    "                                       combine_prompt=COMBINE_PROMPT,\n",
    "                                       return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b412c56-650f-4ca4-a868-9954f83679fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.65 ms, sys: 375 µs, total: 9.02 ms\n",
      "Wall time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Try with other language as well\n",
    "response = chain({\"input_documents\": top_docs, \"question\": QUESTION, \"language\": \"English\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63f07b08-87bd-4518-b2f2-03ee1096f59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the book \"Père riche, Père pauvre\" (Rich Dad, Poor Dad), it is explained that the rich dad and the poor dad have different approaches to money and financial education. The rich dad teaches his children about money and how to make it work for them, while the poor dad does not provide the same financial education. The book explores the different mindsets and strategies for achieving financial success<sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D\">[1]</a></sup><sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D\">[2]</a></sup><sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D\">[3]</a></sup><sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Pere_Riche_Pere_Pauvre.pdf?sv=2022-11-02&ss=b&srt=o&sp=rlytf&se=2024-04-17T01:54:45Z&st=2023-08-10T17:54:45Z&spr=https&sig=nJg%2BFf7rs%2Bjp2syY5BET0GvaXOYjxGJmw36kQgVm7TE%3D\">[4]</a></sup>."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421e786-3bb8-40ba-a962-9b4571a7838f",
   "metadata": {},
   "source": [
    "## 2 - On-Demand Vectorization with Text-based-AI-Enriched Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8adc5-f632-456b-9c3c-6635330a55c4",
   "metadata": {},
   "source": [
    "The last method proved to be highly effective, as it not only solved the challenge of handling large and complex PDF documents but also improved search speed by approximately 10 seconds on average through vector search.\n",
    "\n",
    "However, this method does have its limitations:\n",
    "\n",
    "- It is limited to processing only PDF files.\n",
    "- Because this is a PUSH method, it doesn't use the advantages of the Indexer (PULL method): Scheduler, Change and Delete file detection, automated id key creation, etc.\n",
    "\n",
    "Our ultimate goal is to rely solely on vector indexes to overcome our initial limitations. While it is possible to manually code parsers with OCR for various file types and develop a scheduler to synchronize data with the index, there is a more efficient alternative: **Azure Cognitive Search is soon going to release automated chunking strategies and vectorization within the next months**, so we have three options: \n",
    "1. Wait for this functionality and in the meantime keep embedding on-demand as shown in Notebook 3 \n",
    "2. Create vector-based indexes per each text-based indexes and fill them up on-demand as documents are discovered\n",
    "3. Use custom skills (for chunking and vectorization) and use knowledge stores in order to create a vector-base index from a text-based-ai-enriched index at ingestion time. See [HERE](https://github.com/Azure/cognitive-search-vector-pr/blob/main/demo-python/code/azure-search-vector-ingestion-python-sample.ipynb) for instructions on how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc627c-8c8d-42b1-9052-38dc5696b3f9",
   "metadata": {},
   "source": [
    "Below we are going to try Option 2: **Create vector-based indexes per each text-based indexes and fill them up on-demand as documents are discovered**. Why? because is simpler and quick to implement, while we wait for Optio 1 to become a feature of Azure Search Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339b1c7-3e8c-480c-9ef5-5eec61184524",
   "metadata": {},
   "source": [
    "As you noticed in Notebooks 1 and 2, there is a field in the index called `vectorized` that we have not use yet. Now we will make use of that field. \n",
    "The goal is to NOT vectorize all documents at ingestion time, but instead vectorized the chunks as people search. That way we spend money and resources only when the documents are needed.\n",
    "Normally in an organization with vast amounts of documents in a data lake, 20% of the documents are what people need, the rest are never needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a152176-cda1-4528-91d8-863e4ac2bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom libraries that we will use later in the app\n",
    "from common.utils import (\n",
    "    get_search_results,\n",
    "    order_search_results,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    embed_docs,\n",
    "    search_docs,\n",
    "    get_answer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360db7fa-afdf-46fd-b289-88604ff94fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name = \"cogsrch-index-files\"\n",
    "index2_name = \"cogsrch-index-csv\"\n",
    "indexes = [index1_name, index2_name]\n",
    "\n",
    "agg_search_results = get_search_results(QUESTION, indexes)\n",
    "ordered_results = order_search_results(agg_search_results, reranker_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941796c-7655-4888-a358-8a62e380bd7e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we have acquired an understanding of how to address the challenge of indexing complex or large documents by leveraging the vector search capabilities offered by Azure Cognitive Search.\n",
    "\n",
    "Additionally, we concluded that, until Azure Search introduces automated Chunk Index creation via the Indexer, it would be more straightforward to proceed with an on-the-fly vectorization strategy for the majority of smaller documents stored in the data lake. By doing so, although might seem inefficient for now, we can avoid the manual complexities associated with creating customized skills and maintaining synchronization between the Document Index and Chunk Index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9a7d1-f029-416b-8eb2-00a8afb9151d",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "So far we have learned how to use OpenAI vectors and completion APIs in order to get an excelent answer from our documents stored in Azure Cognitive Search. This is the backbone for a GPT Smart Search Engine.\n",
    "\n",
    "However, we are missing something: **How to have a conversation with this engine?**\n",
    "\n",
    "On the next Notebook, we are going to understand the concept of **memory**. This is necessary in order to have a chatbot that can establish a conversation with the user. Without memory, there is no real conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e083b-96be-465a-b77d-45e43a031b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
